# -*- coding: utf-8 -*-
"""rag_query_builder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Dkpl4RFO42vUQm29jvK7L0jJpZdKAk1
"""

!wget -O wiki_sample.txt https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt
# !wget -O thai_sample.txt https://th.wikipedia.org/wiki/%E0%B8%9B%E0%B8%B1%E0%B8%8D%E0%B8%8D%E0%B8%B2%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%94%E0%B8%B4%E0%B8%A9%E0%B8%90%E0%B9%8C
# (or another small wiki-like article)

# !wget -O attn_paper.pdf https://arxiv.org/pdf/1706.03762.pdf

from huggingface_hub import notebook_login
notebook_login()

# รันครั้งเดียว
!pip install --quiet transformers accelerate sentencepiece huggingface_hub \
    sentence-transformers faiss-cpu PyPDF2 nltk

import os, json, re, math, numpy as np
import faiss, torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")
nltk.download("averaged_perceptron_tagger")
nltk.download("wordnet")
nltk.download("omw-1.4")

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

import re
from PyPDF2 import PdfReader

# ---------- TXT Reader ----------
def read_text(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

# ---------- PDF Reader ----------
def read_pdf(path):
    reader = PdfReader(path)
    pages = []
    for p in reader.pages:
        try:
            pages.append(p.extract_text() or "")
        except:
            pages.append("")
    return "\n".join(pages)

# ---------- NLTK Sentence Splitter (English) ----------
import nltk
from nltk.tokenize import sent_tokenize

def ensure_nltk():
    # punkt
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    # punkt_tab (NLTK >=3.8)
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt_tab')

def text_to_chunks_by_sentence(text, max_words=300, overlap_words=60):
    ensure_nltk()
    sents = sent_tokenize(text)
    chunks = []
    cur, cur_len = [], 0
    for s in sents:
        w = s.split()
        if cur_len + len(w) > max_words and cur:
            chunks.append(" ".join(cur).strip())
            # overlap
            if overlap_words > 0:
                tail = " ".join(" ".join(cur).split()[-overlap_words:])
                cur = tail.split()
                cur_len = len(cur)
            else:
                cur, cur_len = [], 0
        cur.extend(w)
        cur_len += len(w)
    if cur:
        chunks.append(" ".join(cur).strip())
    return chunks

# ---------- Lightweight splitter (Thai / fallback) ----------
def split_sentences_light(text):
    # no lookbehind → safe for all Python versions
    return [s.strip() for s in re.split(r'[.!?]\s+|\n+', text) if s.strip()]

def text_to_chunks_light(text, max_words=250, overlap_words=50):
    sents = split_sentences_light(text)
    chunks = []
    cur, cur_len = [], 0
    for s in sents:
        w = s.split()
        if cur_len + len(w) > max_words and cur:
            chunks.append(" ".join(cur).strip())
            if overlap_words > 0:
                tail = " ".join(" ".join(cur).split()[-overlap_words:])
                cur = tail.split()
                cur_len = len(cur)
            else:
                cur, cur_len = [], 0
        cur.extend(w)
        cur_len += len(w)
    if cur:
        chunks.append(" ".join(cur).strip())
    return chunks

print("Cell C loaded successfully. PDF + TXT + English + Thai chunkers ready.")

# paths
eng_path = "/content/wiki_sample.txt"
thai_path = "/content/20241125-Generative-AI-Guideline_V2-0.pdf"

# โหลดไฟล์ภาษาอังกฤษ
if eng_path.lower().endswith(".pdf"):
    eng_text = read_pdf(eng_path)
else:
    eng_text = read_text(eng_path)

# โหลดภาษาไทย
if thai_path.lower().endswith(".pdf"):
    thai_text = read_pdf(thai_path)
else:
    thai_text = read_text(thai_path)

print("English text length:", len(eng_text))
print(eng_text[:800])

print("\n---\nThai text length:", len(thai_text))
print(thai_text[:800])

# English chunks
eng_chunks = text_to_chunks_by_sentence(eng_text, max_words=200, overlap_words=40)
print("Eng chunks:", len(eng_chunks))
print("Eng chunk[0]:", eng_chunks[0][:300])

# Thai chunks (light splitter)
thai_chunks = text_to_chunks_light(thai_text, max_words=200, overlap_words=40)
print("Thai chunks:", len(thai_chunks))
print("Thai chunk[0]:", thai_chunks[0][:300])

# load embedder
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
print("Embedder ready")

# function to build index
def build_faiss_index_from_chunks(chunks, embedder, batch_size=32):
    vecs = []
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        em = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        vecs.append(em)
    vecs = np.vstack(vecs).astype('float32')
    faiss.normalize_L2(vecs)
    d = vecs.shape[1]
    index = faiss.IndexFlatIP(d)
    index.add(vecs)
    return index, vecs

# build for both
eng_index, eng_vecs = build_faiss_index_from_chunks(eng_chunks, embed_model)
thai_index, thai_vecs = build_faiss_index_from_chunks(thai_chunks, embed_model)
print("Eng index size:", eng_index.ntotal, "Thai index size:", thai_index.ntotal)

def retrieve(index, embedder, chunks, query, top_k=10):
    if index.ntotal == 0:
        return []
    qvec = embedder.encode([query], convert_to_numpy=True).astype('float32')
    faiss.normalize_L2(qvec)
    k = min(top_k, index.ntotal)
    D, I = index.search(qvec, k)
    results = []
    for score, idx in zip(D[0], I[0]):
        if idx < 0 or idx >= len(chunks):
            continue
        results.append((chunks[idx], float(score), int(idx)))
    return results

def mmr(query_embedding, doc_embeddings, docs, top_k=5, lambda_param=0.6):
    sim_to_query = np.dot(doc_embeddings, query_embedding)
    selected = []
    not_selected = list(range(len(docs)))
    if not not_selected:
        return []
    first = int(np.argmax(sim_to_query))
    selected.append(first)
    not_selected.remove(first)
    while len(selected) < min(top_k, len(docs)):
        mmr_score = []
        for idx in not_selected:
            relevance = sim_to_query[idx]
            diversity = max(np.dot(doc_embeddings[idx], doc_embeddings[s]) for s in selected)
            score = lambda_param * relevance - (1-lambda_param) * diversity
            mmr_score.append((score, idx))
        mmr_score.sort(reverse=True)
        chosen = mmr_score[0][1]
        selected.append(chosen)
        not_selected.remove(chosen)
    return selected

query_eng = "Can you tell me about The Adventures of Sherlock Holmes"
candidates = retrieve(eng_index, embed_model, eng_chunks, query_eng, top_k=10)
print("Candidates (eng):")
for i,(txt,score,idx) in enumerate(candidates):
    print(i, "score", round(score,3), "idx", idx, "preview:", txt[:200].replace("\n"," "))

# MMR rerank (use eng_vecs)
cand_indices = [c[2] for c in candidates]
cand_embs = eng_vecs[cand_indices]
qvec = embed_model.encode([query_eng], convert_to_numpy=True).astype('float32')
# normalize
from numpy.linalg import norm
qvec = qvec[0] / (norm(qvec[0]) + 1e-12)
sel_positions = mmr(qvec, cand_embs, [eng_chunks[i] for i in cand_indices], top_k=4, lambda_param=0.6)
selected_indices = [cand_indices[pos] for pos in sel_positions]
print("Selected (MMR):", selected_indices)
for idx in selected_indices:
    print("-> chunk", idx, eng_chunks[idx][:300].replace("\n"," "))

query_th = "องค์ประกอบสำคัญของ AI มีอะไรบ้าง?"
candidates = retrieve(thai_index, embed_model, thai_chunks, query_th, top_k=10)
print("Candidates (thai):")
for i,(txt,score,idx) in enumerate(candidates):
    print(i, "score", round(score,3), "idx", idx, "preview:", txt[:200].replace("\n"," "))

cand_indices = [c[2] for c in candidates]
cand_embs = thai_vecs[cand_indices]
qvec = embed_model.encode([query_th], convert_to_numpy=True).astype('float32')
qvec = qvec[0] / (np.linalg.norm(qvec[0]) + 1e-12)
sel_positions = mmr(qvec, cand_embs, [thai_chunks[i] for i in cand_indices], top_k=4, lambda_param=0.6)
selected_indices = [cand_indices[pos] for pos in sel_positions]
print("Selected (MMR):", selected_indices)
for idx in selected_indices:
    print("-> chunk", idx, thai_chunks[idx][:300].replace("\n"," "))

# loader (safe)
FALLBACK_SEQ2SEQ = "google/flan-t5-small"

def load_llm(model_id="google/gemma-2b-it", token=None):
    try:
        tok = AutoTokenizer.from_pretrained(model_id, token=token)
        m = AutoModelForCausalLM.from_pretrained(model_id, token=token)
        if getattr(m.config, 'pad_token_id', None) is None:
            m.config.pad_token_id = m.config.eos_token_id
            tok.pad_token = tok.eos_token
        m.to(device)
        return tok, m, 'causal'
    except Exception as e:
        print("Causal load failed:", e)
    # fallback
    tok = AutoTokenizer.from_pretrained(FALLBACK_SEQ2SEQ)
    m = AutoModelForSeq2SeqLM.from_pretrained(FALLBACK_SEQ2SEQ)
    if getattr(m.config, 'pad_token_id', None) is None:
        m.config.pad_token_id = m.config.eos_token_id
        tok.pad_token = tok.eos_token
    m.to(device)
    return tok, m, 'seq2seq'

token = os.environ.get('HF_TOKEN', None)  # or None if not logged in
tokenizer, model, model_type = load_llm(token=token)
print("Model type:", model_type)

def build_rag_prompt_from_indices(question, chunks, indices, role='story teller', max_context_chars=2000):
    context = ''
    for idx in indices:
        context += f"[chunk {idx}]\\n" + chunks[idx].strip() + '\\n\\n---\\n\\n'
        if len(context) > max_context_chars:
            break
    prompt = f"You are an expert {role}. Use the following context to answer the question. If the answer is not in the context, say 'I don't know'.\\n\\nContext:\\n{context}\\nQuestion: {question}\\n\\nAnswer concisely."
    return prompt

# Build prompt for English
prompt_eng = build_rag_prompt_from_indices(query_eng, eng_chunks, selected_indices, role='Story teller')
print("PROMPT (eng) preview:\\n", prompt_eng[:800])

# Generate
def generate_from_llm_prompt(prompt, tokenizer, model, model_type='causal', max_new_tokens=150):
    if model_type == 'causal':
        inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.2, top_p=0.95)
        text = tokenizer.decode(out[0], skip_special_tokens=True)
        return text[len(prompt):].strip() if text.startswith(prompt) else text
    else:
        inputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(device)
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
        text = tokenizer.decode(out[0], skip_special_tokens=True)
        return text

ans_eng = generate_from_llm_prompt(prompt_eng, tokenizer, model, model_type=model_type, max_new_tokens=150)
print("=== Answer (eng) ===\\n", ans_eng)

prompt_th = build_rag_prompt_from_indices(query_th, thai_chunks, selected_indices, role='ผู้เชี่ยวชาญด้าน AI')
print("PROMPT (thai) preview:\\n", prompt_th[:800])

ans_th = generate_from_llm_prompt(prompt_th, tokenizer, model, model_type=model_type, max_new_tokens=150)
print("=== Answer (thai) ===\\n", ans_th)